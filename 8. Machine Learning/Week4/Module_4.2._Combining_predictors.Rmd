---
title: "Combining Predictors/Emsembling method"
author: "Katherine Shim"
date: "December 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ISLR); data(Wage); library(ggplot2); library(caret);
```

### Key ideas

* You can combine classifiers by averaging/voting
* Combining classifiers improve accuracy
* Combining classifiers reduces interpretability
* Boosting, bagging, and random forests are variants on this theme.

### Basic intuition - majority vote
Suppose we have 5 completely independent classifiers  
If accuracy is 70% for each:

* $10 x (0.7)^3(0.3)^2 +5 x (0.7)^4(0.3)^1 +(0.7)^5$
* 83.7% majority vote accuracy
  
With 101 independent classifiers
* 99.9% majority vote accuracy.

### Approaches for combining classifiers

1. Bagging, boosting, random forests: Usually combine similar classifiers
2. Combining different classifiers: Model stacking and ensembling.

###Example: wage data
```{r}
Wage <- subset(Wage,select=-c(logwage))
# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
                              p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
dim(training); dim(testing); dim(validation)
```

### Build two different models
We can observe that the result from two different methods don't agree each other and they don't correspond great with the original testing data either.
```{r}
mod1 <- train(wage~., method="glm", data=training)
mod2 <- train(wage~., method="rf", data=training, trControl=trainControl(method="cv"), number=3)
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
```

### Fit a model that combines predictors
```{r}
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
```

## Testing errors
We can see that combined prediction model has the lowest error rate.
```{r}
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
```

## Predict on validation data set

```{r}
pred1V <- predict(mod1,validation)
pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
```

## Evaluate on validation

```{r}
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
```

###Notes

* Even simple blending can be useful
* Typical model for binary/multiclass data
  * Build an odd number of models
  * Predict with each model
  * Predict the class by majority vote
* This can get dramatically more complicated
