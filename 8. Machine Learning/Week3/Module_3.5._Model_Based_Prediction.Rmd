---
title: "Model Baesd Prediction"
author: "Katherine Shim"
date: "December 5, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(caret)
library(ISLR)
library(ggplot2)
```

###Basic idea

1. Assume the data follow a probabilistic model
2. Use Bayes' theorem to identify optimal classifiers

Pros:

* Can take advantage of structure of the data
* May be computationally convenient
* Are reasonably accurate on real problems

Cons:

* Make additional assumptions about the data
* When the model is incorrect you may get reduced accuracy.

### Model based approach

1. Our goal is to build parametric model for conditional distribution $P(Y=k|X=x)$
2. A typical approach is to apply Bayes theorem:
\[ Pr(Y = k | X=x) = \frac{Pr(X=x|Y=k)Pr(Y=k)}{\sum_{\ell=1}^K Pr(X=x |Y = \ell) Pr(Y=\ell)}\]
\[Pr(Y = k | X=x) = \frac{f_k(x) \pi_k}{\sum_{\ell = 1}^K f_{\ell}(x) \pi_{\ell}}\]
3. Typically prior probabilities $\pi_k$ are set in advance.
4. A common choice for $f_k(x) = \frac{1}{\sigma_k \sqrt{2 \pi}}e^{-\frac{(x-\mu_k)^2}{\sigma_k^2}}$, a Gaussian distribution
5. Estimate the parameters ($\mu_k$,$\sigma_k^2$) from the data.
6. Classify to the class with the highest value of $P(Y = k | X = x)$

### Classifying using the model
Classifiers based on the covariance matrix. A range of models use this approach

* Linear discriminant analysis (LDA) assumes $f_k(x)$ is multivariate Gaussian with same covariances
* Quadratic discrimant analysis assumes $f_k(x)$ is multivariate Gaussian with different covariances
* Model based prediction assumes more complicated versions for the covariance matrix 
* Naive Bayes assumes independence between features for model building

### Why called linear discriminant analysis
This classify two group at a time. We draw straight line where probability of being a group is switches over being the other group.
\[log \frac{Pr(Y = k | X=x)}{Pr(Y = j | X=x)}\]
\[= log \frac{f_k(x)}{f_j(x)} + log \frac{\pi_k}{\pi_j}\]
\[= log \frac{\pi_k}{\pi_j} - \frac{1}{2}(\mu_k^T \Sigma^{-1}\mu_k - \mu_j^T \Sigma^{-1}\mu_j)\]
\[+ x^T \Sigma^{-1} (\mu_k - \mu_j)\]

### LDA

* Used for dimentionality reduction
* LDA is based upon the conecpt of searching for a linear combination of predictors that best separate two classes

### Discriminant function


\[\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k \Sigma^{-1}\mu_k + log(\mu_k)\]

* $\sum$ is the covariance matrix
* $\mu_k$ is the mean of class $k$ for all our features
* $x$ is the predictors
* Goal is to pick value of $k$ that will maximize $\delta_k(x)$
* Decide on class based on $\hat{Y}(x) = argmax_k \delta_k(x)$
* We usually estimate parameters with maximum likelihood

### Naive Bayes

Suppose we have many predictors, we would want to model: $P(Y = k | X_1,\ldots,X_m)$

We could use Bayes Theorem to get:

\[P(Y = k | X_1,\ldots,X_m) = \frac{\pi_k P(X_1,\ldots,X_m| Y=k)}{\sum_{\ell = 1}^K P(X_1,\ldots,X_m | Y=k) \pi_{\ell}}\]
\[\propto \pi_k P(X_1,\ldots,X_m| Y=k)\]

This can be written:

$$P(X_1,\ldots,X_m, Y=k) = \pi_k P(X_1 | Y = k)P(X_2,\ldots,X_m | X_1,Y=k)$$
$$ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k) P(X_3,\ldots,X_m | X_1,X_2, Y=k)$$
$$ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k)\ldots P(X_m|X_1\ldots,X_{m-1},Y=k)$$

We could make an assumption that each covariates are independent from each other to write this:

\[\approx \pi_k P(X_1 | Y = k) P(X_2 | Y = k)\ldots P(X_m |,Y=k)\]

###Example: Iris data
```{r}
data(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
#Modelling
#Linear discriminant analysis
modlda <- train(Species ~ .,data=training,method="lda")
#Naive Bayes
modnb <- train(Species ~ ., data=training,method="nb")
plda <- predict(modlda,testing); pnb <- predict(modnb,testing)
table(plda,pnb)
```
###Comparison of results
```{r}
equalPredictions <- (plda==pnb)
qplot(Petal.Width, Sepal.Width, colour=equalPredictions, data=testing)
```

